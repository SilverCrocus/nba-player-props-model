{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from nba_api.stats.endpoints import playergamelog, commonplayerinfo, leaguegamefinder, leaguedashteamstats, leaguedashplayerstats\n",
    "from nba_api.stats.static import players, teams\n",
    "import time \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total players found: 5024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>is_active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76001</td>\n",
       "      <td>Alaa Abdelnaby</td>\n",
       "      <td>Alaa</td>\n",
       "      <td>Abdelnaby</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76002</td>\n",
       "      <td>Zaid Abdul-Aziz</td>\n",
       "      <td>Zaid</td>\n",
       "      <td>Abdul-Aziz</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76003</td>\n",
       "      <td>Kareem Abdul-Jabbar</td>\n",
       "      <td>Kareem</td>\n",
       "      <td>Abdul-Jabbar</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>Mahmoud Abdul-Rauf</td>\n",
       "      <td>Mahmoud</td>\n",
       "      <td>Abdul-Rauf</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1505</td>\n",
       "      <td>Tariq Abdul-Wahad</td>\n",
       "      <td>Tariq</td>\n",
       "      <td>Abdul-Wahad</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            full_name first_name     last_name  is_active\n",
       "0  76001       Alaa Abdelnaby       Alaa     Abdelnaby      False\n",
       "1  76002      Zaid Abdul-Aziz       Zaid    Abdul-Aziz      False\n",
       "2  76003  Kareem Abdul-Jabbar     Kareem  Abdul-Jabbar      False\n",
       "3     51   Mahmoud Abdul-Rauf    Mahmoud    Abdul-Rauf      False\n",
       "4   1505    Tariq Abdul-Wahad      Tariq   Abdul-Wahad      False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all players\n",
    "nba_players = players.get_players()\n",
    "players_df = pd.DataFrame(nba_players)\n",
    "print(f\"Total players found: {len(players_df)}\")\n",
    "players_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total teams found: 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>nickname</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>year_founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1610612737</td>\n",
       "      <td>Atlanta Hawks</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Hawks</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1610612738</td>\n",
       "      <td>Boston Celtics</td>\n",
       "      <td>BOS</td>\n",
       "      <td>Celtics</td>\n",
       "      <td>Boston</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1610612739</td>\n",
       "      <td>Cleveland Cavaliers</td>\n",
       "      <td>CLE</td>\n",
       "      <td>Cavaliers</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1610612740</td>\n",
       "      <td>New Orleans Pelicans</td>\n",
       "      <td>NOP</td>\n",
       "      <td>Pelicans</td>\n",
       "      <td>New Orleans</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1610612741</td>\n",
       "      <td>Chicago Bulls</td>\n",
       "      <td>CHI</td>\n",
       "      <td>Bulls</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             full_name abbreviation   nickname         city  \\\n",
       "0  1610612737         Atlanta Hawks          ATL      Hawks      Atlanta   \n",
       "1  1610612738        Boston Celtics          BOS    Celtics       Boston   \n",
       "2  1610612739   Cleveland Cavaliers          CLE  Cavaliers    Cleveland   \n",
       "3  1610612740  New Orleans Pelicans          NOP   Pelicans  New Orleans   \n",
       "4  1610612741         Chicago Bulls          CHI      Bulls      Chicago   \n",
       "\n",
       "           state  year_founded  \n",
       "0        Georgia          1949  \n",
       "1  Massachusetts          1946  \n",
       "2           Ohio          1970  \n",
       "3      Louisiana          2002  \n",
       "4       Illinois          1966  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all teams\n",
    "nba_teams = teams.get_teams()\n",
    "teams_df = pd.DataFrame(nba_teams)\n",
    "print(f\"Total teams found: {len(teams_df)}\")\n",
    "teams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching team defensive stats...\n",
      "\n",
      "SSL Certificate Error fetching team stats: HTTPSConnectionPool(host='stats.nba.com', port=443): Max retries exceeded with url: /stats/leaguedashteamstats?Conference=&DateFrom=&DateTo=&Division=&GameScope=&GameSegment=&LastNGames=0&LeagueID=&Location=&MeasureType=Defense&Month=0&OpponentTeamID=0&Outcome=&PORound=&PaceAdjust=N&PerMode=Totals&Period=0&PlayerExperience=&PlayerPosition=&PlusMinus=N&Rank=N&Season=2023-24&SeasonSegment=&SeasonType=Regular+Season&ShotClockRange=&StarterBench=&TeamID=&TwoWay=&VsConference=&VsDivision= (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\n",
      "This might be due to a corporate network/proxy. Using fallback DEF_RATING.\n"
     ]
    }
   ],
   "source": [
    "# Fetch Team Defensive Stats for the season\n",
    "print(\"Fetching team defensive stats...\")\n",
    "team_def_stats_df = pd.DataFrame() \n",
    "try:\n",
    "    team_stats = leaguedashteamstats.LeagueDashTeamStats(\n",
    "        season='2023-24',\n",
    "        measure_type_detailed_defense='Defense' \n",
    "    )\n",
    "    temp_df = team_stats.get_data_frames()[0]\n",
    "    \n",
    "    print(\"Available columns in team stats:\")\n",
    "    print(temp_df.columns) \n",
    "\n",
    "    identifier_column = 'TEAM_NAME' \n",
    "    \n",
    "    if identifier_column not in temp_df.columns:\n",
    "         if 'TEAM_ABBREVIATION' in temp_df.columns:\n",
    "              identifier_column = 'TEAM_ABBREVIATION'\n",
    "              print(f\"Using '{identifier_column}' as identifier.\")\n",
    "         elif 'TEAM_ID' in temp_df.columns:\n",
    "              identifier_column = 'TEAM_ID'\n",
    "              print(f\"Using '{identifier_column}' as identifier.\")\n",
    "         else:\n",
    "              raise KeyError(f\"Could not find a suitable team identifier column. Available: {temp_df.columns}\")\n",
    "\n",
    "    team_def_stats_df = temp_df[[identifier_column, 'DEF_RATING']].copy()\n",
    "    \n",
    "    if identifier_column in ['TEAM_NAME', 'TEAM_ID'] and 'teams_df' in locals():\n",
    "         merge_left_col = 'TEAM_NAME' if identifier_column == 'TEAM_NAME' else 'TEAM_ID'\n",
    "         merge_right_col = 'full_name' if identifier_column == 'TEAM_NAME' else 'id'\n",
    "         \n",
    "         if identifier_column == 'TEAM_ID':\n",
    "             team_def_stats_df[identifier_column] = team_def_stats_df[identifier_column].astype(int)\n",
    "             teams_df['id'] = teams_df['id'].astype(int)\n",
    "\n",
    "         team_def_stats_df = pd.merge(team_def_stats_df, teams_df[['id', 'full_name', 'abbreviation']], left_on=merge_left_col, right_on=merge_right_col, how='left')\n",
    "         \n",
    "         if 'abbreviation' in team_def_stats_df.columns:\n",
    "             team_def_stats_df = team_def_stats_df[['abbreviation', 'DEF_RATING']].rename(columns={'abbreviation': 'TEAM_ABBREVIATION'})\n",
    "         else:\n",
    "              print(\"Warning: Could not find 'abbreviation' after merging with teams_df.\")\n",
    "              team_def_stats_df = pd.DataFrame() \n",
    "              \n",
    "    elif identifier_column == 'TEAM_ABBREVIATION':\n",
    "         pass \n",
    "    else:\n",
    "         print(f\"Warning: Identifier column '{identifier_column}' might require manual handling for merging later.\")\n",
    "\n",
    "\n",
    "    if not team_def_stats_df.empty:\n",
    "        print(\"\\nTeam defensive stats processed.\")\n",
    "        print(team_def_stats_df.head())\n",
    "    elif 'TEAM_ABBREVIATION' in temp_df.columns: \n",
    "         print(\"\\nProcessing stats using TEAM_ABBREVIATION directly.\")\n",
    "         team_def_stats_df = temp_df[['TEAM_ABBREVIATION', 'DEF_RATING']].copy()\n",
    "         print(team_def_stats_df.head())\n",
    "    else:\n",
    "         print(\"\\nCould not process team defensive stats correctly.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch specific SSL error if possible, otherwise general exception\n",
    "    if 'CERTIFICATE_VERIFY_FAILED' in str(e):\n",
    "         print(f\"\\nSSL Certificate Error fetching team stats: {e}\")\n",
    "         print(\"This might be due to a corporate network/proxy. Using fallback DEF_RATING.\")\n",
    "    else:\n",
    "         print(f\"\\nError fetching or processing team defensive stats: {e}\")\n",
    "    team_def_stats_df = pd.DataFrame() # Ensure it's empty on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing raw game logs from nba_gamelogs_raw_2023-24.csv...\n",
      "'TOV' column found in existing CSV.\n",
      "\n",
      "Total game logs available: 5193\n",
      "Unique players in logs: 100\n",
      "'FTA' column successfully included.\n",
      "'TOV' column successfully included.\n",
      "   SEASON_ID  Player_ID   Game_ID     GAME_DATE      MATCHUP WL  MIN  FGM  \\\n",
      "0      22023    1630639  22301196  APR 14, 2024    DAL @ OKC  L   24    5   \n",
      "1      22023    1630639  22301181  APR 12, 2024  DAL vs. DET  L   22    1   \n",
      "2      22023    1630639  22301161  APR 10, 2024    DAL @ MIA  W    2    0   \n",
      "3      22023    1630639  22301144  APR 09, 2024    DAL @ CHA  W    4    1   \n",
      "4      22023    1630639  22301097  APR 05, 2024  DAL vs. GSW  W    5    0   \n",
      "\n",
      "   FGA  FG_PCT  ...  DREB  REB  AST  STL  BLK  TOV  PF  PTS  PLUS_MINUS  \\\n",
      "0   13   0.385  ...     4    5    2    0    0    1   1   12         -18   \n",
      "1    6   0.167  ...     5    6    2    0    0    2   0    2          -4   \n",
      "2    0   0.000  ...     2    2    1    0    0    0   0    0           4   \n",
      "3    2   0.500  ...     0    0    0    0    0    1   1    2           2   \n",
      "4    2   0.000  ...     0    0    1    0    0    0   0    0          -1   \n",
      "\n",
      "   VIDEO_AVAILABLE  \n",
      "0                1  \n",
      "1                1  \n",
      "2                1  \n",
      "3                1  \n",
      "4                1  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to get game logs for a player and season with delay\n",
    "def get_player_log(player_id, season='2023-24'):\n",
    "    print(f\"Fetching logs for player {player_id}...\")\n",
    "    try:\n",
    "        # Note: PlayerGameLog endpoint provides FGA, PTS, FTA, and TOV\n",
    "        log = playergamelog.PlayerGameLog(player_id=player_id, season=season)\n",
    "        df = log.get_data_frames()[0]\n",
    "        time.sleep(0.6) # NBA API rate limit\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching logs for player {player_id}: {e}\")\n",
    "        time.sleep(0.6)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --- Define Season and Output File ---\n",
    "SEASON = '2023-24'\n",
    "RAW_GAMELOG_FILE = f'nba_gamelogs_raw_{SEASON}.csv'\n",
    "MIN_MINUTES_THRESHOLD = 15 # Minimum average minutes per game to be included\n",
    "MAX_PLAYERS_TO_FETCH = 100 # Limit players for faster fetching if needed\n",
    "\n",
    "# --- Check if Processed Data Exists and Contains TOV ---\n",
    "FETCH_REQUIRED = False\n",
    "if os.path.exists(RAW_GAMELOG_FILE):\n",
    "    print(f\"Loading existing raw game logs from {RAW_GAMELOG_FILE}...\")\n",
    "    try:\n",
    "        all_gamelogs_df = pd.read_csv(RAW_GAMELOG_FILE)\n",
    "        # Ensure Player_ID is integer if loaded from CSV\n",
    "        if 'Player_ID' in all_gamelogs_df.columns:\n",
    "            all_gamelogs_df['Player_ID'] = all_gamelogs_df['Player_ID'].astype(int)\n",
    "        # Check if TOV column is present\n",
    "        if 'TOV' not in all_gamelogs_df.columns:\n",
    "            print(\"Warning: 'TOV' column missing from existing CSV. Re-fetching required.\")\n",
    "            FETCH_REQUIRED = True\n",
    "        else:\n",
    "            print(\"'TOV' column found in existing CSV.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or checking CSV file {RAW_GAMELOG_FILE}: {e}. Re-fetching required.\")\n",
    "        FETCH_REQUIRED = True\n",
    "        all_gamelogs_df = pd.DataFrame() # Ensure it's empty if loading failed\n",
    "else:\n",
    "    print(\"Raw game log file not found. Fetching data...\")\n",
    "    FETCH_REQUIRED = True\n",
    "    all_gamelogs_df = pd.DataFrame()\n",
    "\n",
    "# --- Fetch Data if Required ---\n",
    "if FETCH_REQUIRED:\n",
    "    # Remove old file if it exists but is incomplete\n",
    "    if os.path.exists(RAW_GAMELOG_FILE):\n",
    "        print(f\"Removing incomplete file: {RAW_GAMELOG_FILE}\")\n",
    "        try:\n",
    "            os.remove(RAW_GAMELOG_FILE)\n",
    "        except OSError as e:\n",
    "            print(f\"Error removing file: {e}\")\n",
    "            \n",
    "    # --- Filter Players Based on Season Stats (e.g., Minutes Played) ---\n",
    "    print(f\"Fetching player stats for {SEASON} to filter...\")\n",
    "    player_ids_to_fetch = []\n",
    "    try:\n",
    "        player_stats = leaguedashplayerstats.LeagueDashPlayerStats(season=SEASON)\n",
    "        player_stats_df = player_stats.get_data_frames()[0]\n",
    "        time.sleep(0.6)\n",
    "        \n",
    "        # Filter players playing significant minutes\n",
    "        relevant_players_df = player_stats_df[player_stats_df['MIN'] >= MIN_MINUTES_THRESHOLD]\n",
    "        player_ids_to_fetch = relevant_players_df['PLAYER_ID'].unique().tolist()\n",
    "        print(f\"Found {len(player_ids_to_fetch)} players averaging >= {MIN_MINUTES_THRESHOLD} MPG.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching player stats for filtering: {e}. Falling back to all active players.\")\n",
    "        # Fallback: Get all active players if stats fetch fails\n",
    "        active_players_df = players_df[players_df['is_active'] == True]\n",
    "        player_ids_to_fetch = active_players_df['id'].tolist()\n",
    "        print(f\"Fetching for all {len(player_ids_to_fetch)} active players (fallback).\")\n",
    "        \n",
    "    # --- Limit players if needed ---\n",
    "    if len(player_ids_to_fetch) > MAX_PLAYERS_TO_FETCH:\n",
    "        print(f\"Limiting fetch to {MAX_PLAYERS_TO_FETCH} players for speed.\")\n",
    "        player_ids_to_fetch = player_ids_to_fetch[:MAX_PLAYERS_TO_FETCH]\n",
    "        \n",
    "    # --- Fetching game logs for filtered players ---\n",
    "    if player_ids_to_fetch:\n",
    "        print(f\"Fetching game logs for {len(player_ids_to_fetch)} players...\")\n",
    "        fetched_logs = [] # Collect dataframes in a list first\n",
    "        for i, p_id in enumerate(player_ids_to_fetch):\n",
    "            print(f\"Progress: {i+1}/{len(player_ids_to_fetch)}\")\n",
    "            player_log_df = get_player_log(p_id, season=SEASON)\n",
    "            if not player_log_df.empty:\n",
    "                # Add Player_ID if it's missing (sometimes happens)\n",
    "                if 'Player_ID' not in player_log_df.columns:\n",
    "                     player_log_df['Player_ID'] = p_id\n",
    "                fetched_logs.append(player_log_df)\n",
    "                \n",
    "        # --- Concatenate and Save the fetched data ---\n",
    "        if fetched_logs:\n",
    "            all_gamelogs_df = pd.concat(fetched_logs, ignore_index=True)\n",
    "            print(f\"\\nSaving {len(all_gamelogs_df)} game logs to {RAW_GAMELOG_FILE}...\")\n",
    "            all_gamelogs_df.to_csv(RAW_GAMELOG_FILE, index=False)\n",
    "            print(\"Save complete.\")\n",
    "        else:\n",
    "            print(\"\\nNo game logs were fetched or concatenated.\")\n",
    "            all_gamelogs_df = pd.DataFrame() # Ensure it's an empty DF if nothing was fetched\n",
    "    else:\n",
    "        print(\"\\nNo player IDs identified for fetching.\")\n",
    "        all_gamelogs_df = pd.DataFrame()\n",
    "\n",
    "# --- Display results ---\n",
    "if not all_gamelogs_df.empty:\n",
    "    print(f\"\\nTotal game logs available: {len(all_gamelogs_df)}\")\n",
    "    print(f\"Unique players in logs: {all_gamelogs_df['Player_ID'].nunique()}\")\n",
    "    # Check essential columns after loading/fetching\n",
    "    for col in ['FTA', 'TOV']:\n",
    "        if col in all_gamelogs_df.columns:\n",
    "            print(f\"'{col}' column successfully included.\")\n",
    "        else:\n",
    "            print(f\"Warning: '{col}' column is missing from the loaded/fetched data!\")\n",
    "    print(all_gamelogs_df.head())\n",
    "else:\n",
    "    print(\"\\nall_gamelogs_df is empty. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing complete.\n",
      "      Player_ID   Game_ID  GAME_DATE      MATCHUP WL  MIN  PTS  REB  AST  \\\n",
      "4303     101108  22300062 2023-10-24  GSW vs. PHX  L   34   14    6    9   \n",
      "4302     101108  22300087 2023-10-27    GSW @ SAC  W   33   10    2   12   \n",
      "4301     101108  22300096 2023-10-29    GSW @ HOU  W   27    8    5    7   \n",
      "4300     101108  22300108 2023-10-30    GSW @ NOP  W   25   13    6    5   \n",
      "4299     101108  22300126 2023-11-01  GSW vs. SAC  W   28    2    4    8   \n",
      "\n",
      "      FG3M  STL  BLK  TOV  FGA  FTA Opponent Home_Away  \n",
      "4303     0    2    0    1   15    7      PHX      Home  \n",
      "4302     0    3    0    3   12    0      SAC      Away  \n",
      "4301     0    1    0    1    8    2      HOU      Away  \n",
      "4300     1    2    0    1   10    0      NOP      Away  \n",
      "4299     0    0    0    0    5    0      SAC      Home  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2133689812.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  processed_df['GAME_DATE'] = pd.to_datetime(processed_df['GAME_DATE'])\n"
     ]
    }
   ],
   "source": [
    "# --- Data Preprocessing ---\n",
    "\n",
    "# Check if all_gamelogs_df exists and is not empty before proceeding\n",
    "if 'all_gamelogs_df' in locals() and not all_gamelogs_df.empty:\n",
    "    processed_df = all_gamelogs_df.copy()\n",
    "    processed_df['GAME_DATE'] = pd.to_datetime(processed_df['GAME_DATE'])\n",
    "\n",
    "    # Select relevant columns (including FGA, FTA, and TOV)\n",
    "    # Ensure all expected columns exist, handle missing ones if necessary\n",
    "    expected_cols = ['Player_ID', 'Game_ID', 'GAME_DATE', 'MATCHUP', 'WL', \n",
    "                     'MIN', 'PTS', 'REB', 'AST', 'FG3M', 'STL', 'BLK', 'TOV', 'FGA', 'FTA'] # Added TOV\n",
    "    available_cols = [col for col in expected_cols if col in processed_df.columns]\n",
    "    missing_cols = [col for col in expected_cols if col not in processed_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing expected columns: {missing_cols}. Proceeding with available columns.\")\n",
    "    processed_df = processed_df[available_cols]\n",
    "\n",
    "    # Ensure necessary columns for calculations are numeric, coercing errors\n",
    "    for col in ['PTS', 'FGA', 'FTA', 'MIN', 'TOV']: # Added TOV\n",
    "        if col in processed_df.columns:\n",
    "            processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')\n",
    "        else:\n",
    "             print(f\"Warning: Column {col} needed for processing is missing.\")\n",
    "             \n",
    "    # Drop rows where essential numeric columns became NaN after coercion\n",
    "    # Important: Check if TOV exists before adding to subset\n",
    "    dropna_subset = ['PTS', 'FGA', 'FTA', 'MIN']\n",
    "    if 'TOV' in processed_df.columns:\n",
    "        dropna_subset.append('TOV')\n",
    "    processed_df.dropna(subset=dropna_subset, inplace=True)\n",
    "\n",
    "    def parse_matchup(matchup_str):\n",
    "        if pd.isna(matchup_str):\n",
    "             return 'Unknown', 'Unknown'\n",
    "        if '@' in matchup_str:\n",
    "            parts = matchup_str.split(' @ ')\n",
    "            opponent = parts[1]\n",
    "            home_away = 'Away'\n",
    "        elif 'vs.' in matchup_str:\n",
    "            parts = matchup_str.split(' vs. ')\n",
    "            opponent = parts[1]\n",
    "            home_away = 'Home'\n",
    "        else: \n",
    "            opponent = 'Unknown'\n",
    "            home_away = 'Unknown'\n",
    "        return opponent, home_away\n",
    "\n",
    "    if 'MATCHUP' in processed_df.columns:\n",
    "        processed_df[['Opponent', 'Home_Away']] = processed_df['MATCHUP'].apply(\n",
    "            lambda x: pd.Series(parse_matchup(x))\n",
    "        )\n",
    "    else:\n",
    "        print(\"Warning: 'MATCHUP' column not found. Cannot determine Opponent or Home/Away.\")\n",
    "        processed_df['Opponent'] = 'Unknown'\n",
    "        processed_df['Home_Away'] = 'Unknown'\n",
    "\n",
    "    processed_df = processed_df.sort_values(by=['Player_ID', 'GAME_DATE'])\n",
    "\n",
    "    print(\"Data preprocessing complete.\")\n",
    "    print(processed_df.head())\n",
    "else:\n",
    "    print(\"Skipping Data Preprocessing because 'all_gamelogs_df' is not available or empty.\")\n",
    "    processed_df = pd.DataFrame() # Ensure processed_df exists even if empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df['Rest_Days'].fillna(2, inplace=True) # Assuming 2 days rest for the first game as a default\n",
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df['Player_USG_Proxy'].fillna(0, inplace=True)\n",
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df['Player_USG_Proxy'].replace([np.inf, -np.inf], 0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Feature Engineering...\n",
      "Calculated Rest Days.\n",
      "Calculated Player Usage Rate Proxy.\n",
      "Calculated TS%.\n",
      "Calculated cumulative averages including TS% and USG% Proxy.\n",
      "Warning: Team defensive stats not available or Opponent column missing. Using default DEF_RATING: 115.0\n",
      "Feature Engineering complete.\n",
      "      Player_ID   Game_ID  GAME_DATE      MATCHUP WL  MIN  PTS  REB  AST  \\\n",
      "4303     101108  22300062 2023-10-24  GSW vs. PHX  L   34   14    6    9   \n",
      "4302     101108  22300087 2023-10-27    GSW @ SAC  W   33   10    2   12   \n",
      "4301     101108  22300096 2023-10-29    GSW @ HOU  W   27    8    5    7   \n",
      "4300     101108  22300108 2023-10-30    GSW @ NOP  W   25   13    6    5   \n",
      "4299     101108  22300126 2023-11-01  GSW vs. SAC  W   28    2    4    8   \n",
      "4298     101108  22300005 2023-11-03    GSW @ OKC  W   28    1    2   13   \n",
      "4297     101108  22300142 2023-11-05    GSW @ CLE  L   25    5    1    2   \n",
      "4296     101108  22300145 2023-11-06    GSW @ DET  W   21   17    5    6   \n",
      "4295     101108  22300169 2023-11-08    GSW @ DEN  L   27    9    5    4   \n",
      "4294     101108  22300176 2023-11-11  GSW vs. CLE  L   26    9    3    9   \n",
      "\n",
      "      FG3M  ...  Cum_Games  Cum_FGA  Cum_FTA  Cum_TOV  Avg_PTS_Season  \\\n",
      "4303     0  ...          0      NaN      NaN      NaN        0.000000   \n",
      "4302     0  ...          1     15.0      7.0      1.0       14.000000   \n",
      "4301     0  ...          2     27.0      7.0      4.0       12.000000   \n",
      "4300     1  ...          3     35.0      9.0      5.0       10.666667   \n",
      "4299     0  ...          4     45.0      9.0      6.0       11.250000   \n",
      "4298     0  ...          5     50.0      9.0      6.0        9.400000   \n",
      "4297     1  ...          6     56.0     11.0      6.0        8.000000   \n",
      "4296     2  ...          7     66.0     11.0      6.0        7.571429   \n",
      "4295     1  ...          8     75.0     14.0      6.0        8.750000   \n",
      "4294     1  ...          9     85.0     14.0      7.0        8.777778   \n",
      "\n",
      "     PTS_Per36_Season Avg_TS%_Season  Avg_USG%_Proxy_Season  Is_Home  \\\n",
      "4303         0.000000       0.000000               0.000000        1   \n",
      "4302        14.823529       0.387168               0.561176        0   \n",
      "4301        12.895522       0.398936               0.508657        0   \n",
      "4300        12.255319       0.410678               0.467660        0   \n",
      "4299        13.613445       0.459559               0.461849        1   \n",
      "4298        11.510204       0.435508               0.407891        0   \n",
      "4297         9.874286       0.394477               0.381943        0   \n",
      "4296         9.540000       0.374082               0.384200        0   \n",
      "4295        11.402715       0.431247               0.394389        0   \n",
      "4294        11.467742       0.433304               0.395806        1   \n",
      "\n",
      "      Opponent_DEF_RATING  \n",
      "4303                115.0  \n",
      "4302                115.0  \n",
      "4301                115.0  \n",
      "4300                115.0  \n",
      "4299                115.0  \n",
      "4298                115.0  \n",
      "4297                115.0  \n",
      "4296                115.0  \n",
      "4295                115.0  \n",
      "4294                115.0  \n",
      "\n",
      "[10 rows x 44 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df['TS%'].fillna(0, inplace=True)\n",
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:33: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df['TS%'].replace([np.inf, -np.inf], 0, inplace=True)\n",
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df[col].fillna(0, inplace=True) # Fill NaNs introduced by coercion\n",
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df[col].fillna(0, inplace=True) # Fill NaNs introduced by coercion\n",
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df[col].fillna(0, inplace=True) # Fill NaNs introduced by coercion\n",
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df[col].fillna(0, inplace=True) # Fill NaNs introduced by coercion\n",
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df[col].fillna(0, inplace=True) # Fill NaNs introduced by coercion\n",
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df[col].fillna(0, inplace=True) # Fill NaNs introduced by coercion\n",
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:73: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df['Avg_TS%_Season'].fillna(0, inplace=True)\n",
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:74: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df['Avg_TS%_Season'].replace([np.inf, -np.inf], 0, inplace=True)\n",
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:80: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df['Avg_USG%_Proxy_Season'].fillna(0, inplace=True)\n",
      "/var/folders/br/0zcbkj2n1nn4rtl2hghl4wvjg03xjf/T/ipykernel_7981/2659041986.py:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  processed_df['Avg_USG%_Proxy_Season'].replace([np.inf, -np.inf], 0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# --- Feature Engineering ---\n",
    "\n",
    "# Check if processed_df exists and is not empty\n",
    "if 'processed_df' in locals() and not processed_df.empty:\n",
    "    print(\"Starting Feature Engineering...\")\n",
    "    \n",
    "    # Calculate Rest Days\n",
    "    processed_df['Rest_Days'] = processed_df.groupby('Player_ID')['GAME_DATE'].diff().dt.days\n",
    "    # Fill first game NaN with a reasonable value (e.g., average rest or a specific indicator)\n",
    "    processed_df['Rest_Days'].fillna(2, inplace=True) # Assuming 2 days rest for the first game as a default\n",
    "    print(\"Calculated Rest Days.\")\n",
    "\n",
    "    # Calculate Player Usage Rate Proxy (Requires TOV)\n",
    "    if 'FGA' in processed_df.columns and 'FTA' in processed_df.columns and 'TOV' in processed_df.columns and 'MIN' in processed_df.columns:\n",
    "        # Ensure TOV is numeric (should be from preprocessing, but double-check)\n",
    "        processed_df['TOV'] = pd.to_numeric(processed_df['TOV'], errors='coerce').fillna(0)\n",
    "        \n",
    "        usg_numerator = processed_df['FGA'] + 0.44 * processed_df['FTA'] + processed_df['TOV']\n",
    "        usg_denominator = processed_df['MIN']\n",
    "        processed_df['Player_USG_Proxy'] = np.where(usg_denominator == 0, 0, usg_numerator / usg_denominator)\n",
    "        processed_df['Player_USG_Proxy'].fillna(0, inplace=True)\n",
    "        processed_df['Player_USG_Proxy'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        print(\"Calculated Player Usage Rate Proxy.\")\n",
    "    else:\n",
    "        print(\"Warning: Could not calculate Player Usage Rate Proxy due to missing FGA, FTA, TOV, or MIN columns.\")\n",
    "        processed_df['Player_USG_Proxy'] = 0 # Assign default value\n",
    "        \n",
    "    # Calculate True Shooting Percentage (TS%)\n",
    "    if 'PTS' in processed_df.columns and 'FGA' in processed_df.columns and 'FTA' in processed_df.columns:\n",
    "        denominator = 2 * (processed_df['FGA'] + 0.44 * processed_df['FTA'])\n",
    "        processed_df['TS%'] = np.where(denominator == 0, 0, processed_df['PTS'] / denominator)\n",
    "        processed_df['TS%'].fillna(0, inplace=True)\n",
    "        processed_df['TS%'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        print(\"Calculated TS%.\")\n",
    "    else:\n",
    "        print(\"Warning: Could not calculate TS% due to missing PTS, FGA, or FTA columns.\")\n",
    "        processed_df['TS%'] = 0 # Assign default value\n",
    "\n",
    "    # Rolling Averages (Include new features)\n",
    "    cols_for_rolling = ['PTS', 'MIN', 'FGA', 'FTA', 'TS%', 'Player_USG_Proxy'] # Added Player_USG_Proxy\n",
    "    for col in cols_for_rolling:\n",
    "        if col in processed_df.columns:\n",
    "            # Ensure column is numeric before rolling calculation\n",
    "            processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')\n",
    "            processed_df[col].fillna(0, inplace=True) # Fill NaNs introduced by coercion\n",
    "            \n",
    "            processed_df[f'{col}_Roll_3'] = processed_df.groupby('Player_ID')[col].transform(\n",
    "                lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)\n",
    "            )\n",
    "            processed_df[f'{col}_Roll_5'] = processed_df.groupby('Player_ID')[col].transform(\n",
    "                lambda x: x.rolling(window=5, min_periods=1).mean().shift(1)\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found for rolling average calculation.\")\n",
    "\n",
    "    # Cumulative Season Averages (Shifted)\n",
    "    # Check required columns exist (including TOV for USG% calculation)\n",
    "    cum_avg_req_cols = ['PTS', 'MIN', 'TS%', 'FGA', 'FTA', 'TOV']\n",
    "    if all(col in processed_df.columns for col in cum_avg_req_cols):\n",
    "        processed_df['Cum_PTS'] = processed_df.groupby('Player_ID')['PTS'].transform(lambda x: x.expanding().sum().shift(1))\n",
    "        processed_df['Cum_MIN'] = processed_df.groupby('Player_ID')['MIN'].transform(lambda x: x.expanding().sum().shift(1))\n",
    "        processed_df['Cum_Games'] = processed_df.groupby('Player_ID').cumcount() \n",
    "        processed_df['Cum_FGA'] = processed_df.groupby('Player_ID')['FGA'].transform(lambda x: x.expanding().sum().shift(1))\n",
    "        processed_df['Cum_FTA'] = processed_df.groupby('Player_ID')['FTA'].transform(lambda x: x.expanding().sum().shift(1))\n",
    "        processed_df['Cum_TOV'] = processed_df.groupby('Player_ID')['TOV'].transform(lambda x: x.expanding().sum().shift(1))\n",
    "\n",
    "        processed_df['Avg_PTS_Season'] = (processed_df['Cum_PTS'] / processed_df['Cum_Games']).replace([np.inf, -np.inf, np.nan], 0)\n",
    "        processed_df['PTS_Per36_Season'] = (processed_df['Cum_PTS'] / processed_df['Cum_MIN'] * 36).replace([np.inf, -np.inf, np.nan], 0)\n",
    "        \n",
    "        # Cumulative TS% calculation\n",
    "        cum_ts_denominator = 2 * (processed_df['Cum_FGA'] + 0.44 * processed_df['Cum_FTA'])\n",
    "        processed_df['Avg_TS%_Season'] = np.where(cum_ts_denominator == 0, 0, processed_df['Cum_PTS'] / cum_ts_denominator)\n",
    "        processed_df['Avg_TS%_Season'].fillna(0, inplace=True)\n",
    "        processed_df['Avg_TS%_Season'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        \n",
    "        # Cumulative USG% Proxy calculation\n",
    "        cum_usg_numerator = processed_df['Cum_FGA'] + 0.44 * processed_df['Cum_FTA'] + processed_df['Cum_TOV']\n",
    "        cum_usg_denominator = processed_df['Cum_MIN']\n",
    "        processed_df['Avg_USG%_Proxy_Season'] = np.where(cum_usg_denominator == 0, 0, cum_usg_numerator / cum_usg_denominator)\n",
    "        processed_df['Avg_USG%_Proxy_Season'].fillna(0, inplace=True)\n",
    "        processed_df['Avg_USG%_Proxy_Season'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        \n",
    "        print(\"Calculated cumulative averages including TS% and USG% Proxy.\")\n",
    "    else:\n",
    "         missing_cum_cols = [col for col in cum_avg_req_cols if col not in processed_df.columns]\n",
    "         print(f\"Warning: One or more columns ({missing_cum_cols}) missing for cumulative average calculation.\")\n",
    "\n",
    "    # Other Features\n",
    "    if 'Home_Away' in processed_df.columns:\n",
    "        processed_df['Is_Home'] = processed_df['Home_Away'].apply(lambda x: 1 if x == 'Home' else 0)\n",
    "    else:\n",
    "        processed_df['Is_Home'] = 0 # Default if Home_Away is missing\n",
    "\n",
    "    # Merge Opponent Stats\n",
    "    # Use fallback DEF_RATING if team_def_stats_df is empty (due to fetch error)\n",
    "    DEFAULT_DEF_RATING = 115.0 \n",
    "    if 'team_def_stats_df' in locals() and not team_def_stats_df.empty and 'TEAM_ABBREVIATION' in team_def_stats_df.columns and 'Opponent' in processed_df.columns:\n",
    "        print(\"Merging fetched team defensive stats...\")\n",
    "        team_def_stats_to_merge = team_def_stats_df.rename(columns={\n",
    "            'TEAM_ABBREVIATION': 'Opponent',\n",
    "            'DEF_RATING': 'Opponent_DEF_RATING'\n",
    "        })\n",
    "        try:\n",
    "            processed_df['Opponent'] = processed_df['Opponent'].astype(team_def_stats_to_merge['Opponent'].dtype)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not align Opponent column types for merge: {e}\")\n",
    "            \n",
    "        processed_df = pd.merge(processed_df, team_def_stats_to_merge[['Opponent', 'Opponent_DEF_RATING']], on='Opponent', how='left')\n",
    "        \n",
    "        if 'Opponent_DEF_RATING' in processed_df.columns:\n",
    "             processed_df['Opponent_DEF_RATING'] = pd.to_numeric(processed_df['Opponent_DEF_RATING'], errors='coerce')\n",
    "             # Fill NaNs with the mean *of the successfully merged ratings* if possible, otherwise use default\n",
    "             avg_def_rating = processed_df['Opponent_DEF_RATING'].mean() \n",
    "             fill_value = avg_def_rating if not pd.isna(avg_def_rating) else DEFAULT_DEF_RATING\n",
    "             processed_df['Opponent_DEF_RATING'].fillna(fill_value, inplace=True)\n",
    "             print(f\"Opponent defensive stats merged. Filled NaNs with {fill_value:.1f}.\")\n",
    "        else:\n",
    "             print(\"Warning: 'Opponent_DEF_RATING' column not created after merge. Using default.\")\n",
    "             processed_df['Opponent_DEF_RATING'] = DEFAULT_DEF_RATING\n",
    "    else:\n",
    "        print(f\"Warning: Team defensive stats not available or Opponent column missing. Using default DEF_RATING: {DEFAULT_DEF_RATING}\")\n",
    "        processed_df['Opponent_DEF_RATING'] = DEFAULT_DEF_RATING\n",
    "\n",
    "    # Final Fill NA for engineered features\n",
    "    # Identify all potential feature columns created\n",
    "    feature_cols = [col for col in processed_df.columns if '_Roll_' in col or '_Season' in col or col == 'Is_Home' or col == 'Opponent_DEF_RATING' or col == 'Rest_Days']\n",
    "    processed_df[feature_cols] = processed_df[feature_cols].fillna(0)\n",
    "    print(\"Feature Engineering complete.\")\n",
    "    print(processed_df.head(10))\n",
    "else:\n",
    "    print(\"Skipping Feature Engineering because 'processed_df' is not available or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for modeling...\n",
      "Using features: ['PTS_Roll_3', 'PTS_Roll_5', 'MIN_Roll_3', 'MIN_Roll_5', 'FGA_Roll_3', 'FGA_Roll_5', 'Avg_PTS_Season', 'PTS_Per36_Season', 'Opponent_DEF_RATING', 'TS%_Roll_3', 'TS%_Roll_5', 'Avg_TS%_Season', 'Player_USG_Proxy_Roll_3', 'Player_USG_Proxy_Roll_5', 'Avg_USG%_Proxy_Season', 'Rest_Days', 'Is_Home']\n",
      "Data prepared for modeling. Training set size: 4154, Testing set size: 1039\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare Data for Modeling ---\n",
    "\n",
    "# Check if processed_df exists and is not empty\n",
    "if 'processed_df' in locals() and not processed_df.empty:\n",
    "    print(\"Preparing data for modeling...\")\n",
    "    # Define required columns based on features actually created\n",
    "    required_cols = []\n",
    "    # Add base features\n",
    "    base_features = ['PTS_Roll_3', 'PTS_Roll_5', 'MIN_Roll_3', 'MIN_Roll_5', \n",
    "                     'FGA_Roll_3', 'FGA_Roll_5', 'Avg_PTS_Season', 'PTS_Per36_Season',\n",
    "                     'Opponent_DEF_RATING']\n",
    "    for col in base_features:\n",
    "        if col in processed_df.columns:\n",
    "            required_cols.append(col)\n",
    "        else:\n",
    "            print(f\"Note: Feature '{col}' not found in processed_df, excluding from model requirements.\")\n",
    "            \n",
    "    # Add TS% features\n",
    "    ts_features = ['TS%_Roll_3', 'TS%_Roll_5', 'Avg_TS%_Season']\n",
    "    for col in ts_features:\n",
    "         if col in processed_df.columns:\n",
    "            required_cols.append(col)\n",
    "         else:\n",
    "            print(f\"Note: TS% Feature '{col}' not found in processed_df, excluding from model requirements.\")\n",
    "            \n",
    "    # Add new USG% and Rest features\n",
    "    new_features = ['Player_USG_Proxy_Roll_3', 'Player_USG_Proxy_Roll_5', 'Avg_USG%_Proxy_Season', 'Rest_Days']\n",
    "    for col in new_features:\n",
    "        if col in processed_df.columns:\n",
    "            required_cols.append(col)\n",
    "        else:\n",
    "            print(f\"Note: New Feature '{col}' not found in processed_df, excluding from model requirements.\")\n",
    "            \n",
    "    # Add target variable if it exists\n",
    "    target = 'PTS'\n",
    "    if target not in processed_df.columns:\n",
    "        print(f\"Error: Target variable '{target}' not found in processed_df. Cannot proceed with modeling.\")\n",
    "        model_df = pd.DataFrame()\n",
    "        X_train, X_test, y_train, y_test = [None]*4\n",
    "    else:\n",
    "        # Drop rows where target or any required feature is missing BEFORE creating model_df\n",
    "        # Also drop rows with insufficient history for rolling features (implicitly handled by fillna(0) then dropna)\n",
    "        model_df = processed_df.dropna(subset=[target] + required_cols).copy()\n",
    "\n",
    "        # Convert Opponent_DEF_RATING to numeric if it exists and wasn't already\n",
    "        if 'Opponent_DEF_RATING' in model_df.columns:\n",
    "            model_df['Opponent_DEF_RATING'] = pd.to_numeric(model_df['Opponent_DEF_RATING'], errors='coerce')\n",
    "            if model_df['Opponent_DEF_RATING'].isnull().any():\n",
    "                mean_def_rating = model_df['Opponent_DEF_RATING'].mean()\n",
    "                print(f\"Filling NaN Opponent_DEF_RATING with mean: {mean_def_rating}\")\n",
    "                model_df['Opponent_DEF_RATING'].fillna(mean_def_rating, inplace=True)\n",
    "        \n",
    "        # Check if enough data remains\n",
    "        if model_df.empty or len(model_df) < 10: # Arbitrary threshold for minimum data\n",
    "            print(\"Not enough data with required features and target to build a model.\")\n",
    "            X_train, X_test, y_train, y_test = [None]*4 \n",
    "        else:\n",
    "            # Define features based on columns actually present in model_df\n",
    "            features = required_cols + ['Is_Home'] # Add Is_Home if it exists\n",
    "            features = [f for f in features if f in model_df.columns] # Ensure all features exist\n",
    "            \n",
    "            print(f\"Using features: {features}\")\n",
    "            X = model_df[features]\n",
    "            y = model_df[target]\n",
    "            \n",
    "            # Check for NaN/inf in features or target before split\n",
    "            if X.isnull().values.any() or y.isnull().values.any() or np.isinf(X.values).any() or np.isinf(y.values).any():\n",
    "                 print(\"Warning: NaN or Inf values detected in features or target before train/test split. Attempting to fill with 0.\")\n",
    "                 X = X.fillna(0)\n",
    "                 y = y.fillna(0)\n",
    "                 X = X.replace([np.inf, -np.inf], 0)\n",
    "                 y = y.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "            \n",
    "            print(f\"Data prepared for modeling. Training set size: {X_train.shape[0]}, Testing set size: {X_test.shape[0]}\")\n",
    "else:\n",
    "    print(\"Skipping Data Preparation for Modeling because 'processed_df' is not available or empty.\")\n",
    "    X_train, X_test, y_train, y_test = [None]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Evaluation Function ---\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name, X_train_cols=None):\n",
    "    \"\"\"Calculates and prints evaluation metrics for a given model.\"\"\"\n",
    "    print(f\"\\n--- {model_name} Evaluation ---\")\n",
    "    \n",
    "    if model is None or X_test is None or y_test is None or X_test.empty:\n",
    "        print(f\"Skipping {model_name} evaluation as model was not trained or test data was missing.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred) \n",
    "        rmse = np.sqrt(mse) \n",
    "\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\") \n",
    "\n",
    "        # --- Basic Error Analysis ---\n",
    "        X_test_results = X_test.copy()\n",
    "        X_test_results['Actual_PTS'] = y_test\n",
    "        X_test_results['Predicted_PTS'] = y_pred\n",
    "        X_test_results['Error'] = X_test_results['Actual_PTS'] - X_test_results['Predicted_PTS']\n",
    "        X_test_results['Abs_Error'] = np.abs(X_test_results['Error'])\n",
    "        \n",
    "        print(f\"\\nSample Predictions vs Actual ({model_name}):\")\n",
    "        print(X_test_results.head())\n",
    "        \n",
    "        print(\"\\nLargest Errors (Top 5):\")\n",
    "        print(X_test_results.sort_values(by='Abs_Error', ascending=False).head())\n",
    "        \n",
    "        # Optional: Feature Importances (Specific to tree-based models like XGBoost)\n",
    "        if hasattr(model, 'feature_importances_') and X_train_cols is not None:\n",
    "            print(f\"\\nFeature Importances ({model_name}):\")\n",
    "            importances = pd.DataFrame({\n",
    "                'Feature': X_train_cols,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            print(importances)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during {model_name} evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Ridge Regression Model...\n",
      "Best alpha found for Ridge: 10.0\n",
      "Tuned Ridge Regression Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Model Building & Tuning (Ridge Regression) ---\n",
    "model_ridge = None\n",
    "if X_train is not None and y_train is not None and not X_train.empty:\n",
    "    print(\"Tuning Ridge Regression Model...\")\n",
    "    \n",
    "    param_grid_ridge = {'alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0]}\n",
    "    \n",
    "    ridge_estimator = Ridge()\n",
    "    grid_search_ridge = GridSearchCV(ridge_estimator, param_grid_ridge, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    \n",
    "    try:\n",
    "        grid_search_ridge.fit(X_train, y_train)\n",
    "        model_ridge = grid_search_ridge.best_estimator_\n",
    "        print(f\"Best alpha found for Ridge: {grid_search_ridge.best_params_['alpha']}\")\n",
    "        print(\"Tuned Ridge Regression Model training complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Ridge GridSearchCV: {e}\")\n",
    "        model_ridge = None # Ensure model is None if fitting fails\n",
    "else:\n",
    "    print(\"Skipping Ridge tuning due to lack of training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuned Ridge Evaluation ---\n",
      "Mean Absolute Error (MAE): 4.50\n",
      "Root Mean Squared Error (RMSE): 5.91\n",
      "\n",
      "Sample Predictions vs Actual (Tuned Ridge):\n",
      "      PTS_Roll_3  PTS_Roll_5  MIN_Roll_3  MIN_Roll_5  FGA_Roll_3  FGA_Roll_5  \\\n",
      "3518   21.333333        19.8   28.666667        27.8   15.000000        15.0   \n",
      "494     7.666667         9.0   24.333333        27.4    5.000000         5.4   \n",
      "2834    3.333333         3.0    5.666667         4.8    3.333333         2.8   \n",
      "2708   19.333333        15.6   34.333333        29.4   16.666667        14.4   \n",
      "4861    2.666667         1.6    4.333333         3.6    1.333333         1.0   \n",
      "\n",
      "      Avg_PTS_Season  PTS_Per36_Season  Opponent_DEF_RATING  TS%_Roll_3  ...  \\\n",
      "3518       20.593750         25.871320                115.0    0.613634  ...   \n",
      "494         7.380952         10.127042                115.0    0.679239  ...   \n",
      "2834        2.142857         19.285714                115.0    0.339793  ...   \n",
      "2708       16.750000         18.843750                115.0    0.521528  ...   \n",
      "4861        2.153846         10.500000                115.0    0.280112  ...   \n",
      "\n",
      "      Avg_TS%_Season  Player_USG_Proxy_Roll_3  Player_USG_Proxy_Roll_5  \\\n",
      "3518        0.535738                 0.612045                 0.625996   \n",
      "494         0.622390                 0.229899                 0.241470   \n",
      "2834        0.456204                 1.086154                 0.851692   \n",
      "2708        0.540366                 0.596901                 0.610095   \n",
      "4861        0.555556                 0.497778                 0.432000   \n",
      "\n",
      "      Avg_USG%_Proxy_Season  Rest_Days  Is_Home  Actual_PTS  Predicted_PTS  \\\n",
      "3518               0.724144        2.0        0          20      20.167283   \n",
      "494                0.262287        3.0        1           8       7.528251   \n",
      "2834               0.658571        3.0        1           5       3.173122   \n",
      "2708               0.539512        2.0        1          31      18.185779   \n",
      "4861               0.314583        2.0        1           2       3.138280   \n",
      "\n",
      "          Error  Abs_Error  \n",
      "3518  -0.167283   0.167283  \n",
      "494    0.471749   0.471749  \n",
      "2834   1.826878   1.826878  \n",
      "2708  12.814221  12.814221  \n",
      "4861  -1.138280   1.138280  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Largest Errors (Top 5):\n",
      "      PTS_Roll_3  PTS_Roll_5  MIN_Roll_3  MIN_Roll_5  FGA_Roll_3  FGA_Roll_5  \\\n",
      "3550    0.000000         0.0    0.000000         0.0    0.000000         0.0   \n",
      "2434    4.666667         2.8   10.666667         7.6    5.333333         3.2   \n",
      "2311   18.000000        17.2   30.333333        30.8   16.333333        15.0   \n",
      "1726   12.000000        12.6   35.666667        36.2   12.000000        11.8   \n",
      "1459   31.666667        32.2   35.666667        36.6   22.000000        22.0   \n",
      "\n",
      "      Avg_PTS_Season  PTS_Per36_Season  Opponent_DEF_RATING  TS%_Roll_3  ...  \\\n",
      "3550        0.000000          0.000000                115.0    0.000000  ...   \n",
      "2434        5.685714         15.373391                115.0    0.462963  ...   \n",
      "2311       16.142857         21.373030                115.0    0.535594  ...   \n",
      "1726       10.876923         14.069652                115.0    0.441276  ...   \n",
      "1459       26.843750         27.734529                115.0    0.624094  ...   \n",
      "\n",
      "      Avg_TS%_Season  Player_USG_Proxy_Roll_3  Player_USG_Proxy_Roll_5  \\\n",
      "3550        0.000000                 0.000000                 0.000000   \n",
      "2434        0.467312                 0.497899                 0.348739   \n",
      "2311        0.594320                 0.597379                 0.548032   \n",
      "1726        0.585614                 0.443120                 0.406223   \n",
      "1459        0.584863                 0.829790                 0.814927   \n",
      "\n",
      "      Avg_USG%_Proxy_Season  Rest_Days  Is_Home  Actual_PTS  Predicted_PTS  \\\n",
      "3550               0.000000        2.0        1          36       5.400210   \n",
      "2434               0.540601        1.0        1          37       6.691931   \n",
      "2311               0.546760        3.0        1          40      17.142881   \n",
      "1726               0.379016        2.0        1          34      12.773637   \n",
      "1459               0.756377        2.0        0           6      27.103594   \n",
      "\n",
      "          Error  Abs_Error  \n",
      "3550  30.599790  30.599790  \n",
      "2434  30.308069  30.308069  \n",
      "2311  22.857119  22.857119  \n",
      "1726  21.226363  21.226363  \n",
      "1459 -21.103594  21.103594  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Model Evaluation (Tuned Ridge Regression) ---\n",
    "evaluate_model(model_ridge, X_test, y_test, \"Tuned Ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuning XGBoost Model ---\n",
      "Starting XGBoost GridSearchCV (this may take a while)...\n",
      "Fitting 5 folds for each of 2187 candidates, totalling 10935 fits\n",
      "\n",
      "Best parameters found for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 1.5, 'subsample': 0.9}\n",
      "Best MAE score during CV: 4.66\n",
      "Tuned XGBoost Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Model Building & Tuning (XGBoost) ---\n",
    "model_xgb_tuned = None\n",
    "if X_train is not None and y_train is not None and not X_train.empty:\n",
    "    print(\"\\n--- Tuning XGBoost Model ---\")\n",
    "    \n",
    "    # Define an expanded parameter grid for more thorough tuning\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [100, 200, 300],       # More trees\n",
    "        'max_depth': [3, 5, 7],                # Deeper trees\n",
    "        'learning_rate': [0.01, 0.05, 0.1],    # Smaller learning rates\n",
    "        'subsample': [0.7, 0.9, 1.0],          # Vary sample fraction\n",
    "        'colsample_bytree': [0.7, 0.9, 1.0],   # Vary feature fraction\n",
    "        'reg_alpha': [0, 0.1, 0.5],            # L1 Regularization\n",
    "        'reg_lambda': [0.5, 1.0, 1.5]          # L2 Regularization\n",
    "    }\n",
    "    \n",
    "    xgb_estimator = XGBRegressor(random_state=42, objective='reg:squarederror') \n",
    "    \n",
    "    # Use GridSearchCV with more folds\n",
    "    grid_search_xgb = GridSearchCV(\n",
    "        estimator=xgb_estimator,\n",
    "        param_grid=param_grid_xgb,\n",
    "        scoring='neg_mean_absolute_error', # Optimize for MAE\n",
    "        cv=5, # Use 5-fold CV for more robust evaluation\n",
    "        n_jobs=-1, # Use all available CPU cores\n",
    "        verbose=1 # Print progress\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting XGBoost GridSearchCV (this may take a while)...\")\n",
    "        grid_search_xgb.fit(X_train, y_train)\n",
    "        model_xgb_tuned = grid_search_xgb.best_estimator_\n",
    "        print(f\"\\nBest parameters found for XGBoost: {grid_search_xgb.best_params_}\")\n",
    "        print(f\"Best MAE score during CV: {-grid_search_xgb.best_score_:.2f}\")\n",
    "        print(\"Tuned XGBoost Model training complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during XGBoost GridSearchCV: {e}\")\n",
    "        model_xgb_tuned = None # Ensure model is None if fitting fails\n",
    "else:\n",
    "    print(\"Skipping XGBoost tuning due to lack of training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuned XGBoost Evaluation ---\n",
      "Mean Absolute Error (MAE): 4.55\n",
      "Root Mean Squared Error (RMSE): 5.97\n",
      "\n",
      "Sample Predictions vs Actual (Tuned XGBoost):\n",
      "      PTS_Roll_3  PTS_Roll_5  MIN_Roll_3  MIN_Roll_5  FGA_Roll_3  FGA_Roll_5  \\\n",
      "3518   21.333333        19.8   28.666667        27.8   15.000000        15.0   \n",
      "494     7.666667         9.0   24.333333        27.4    5.000000         5.4   \n",
      "2834    3.333333         3.0    5.666667         4.8    3.333333         2.8   \n",
      "2708   19.333333        15.6   34.333333        29.4   16.666667        14.4   \n",
      "4861    2.666667         1.6    4.333333         3.6    1.333333         1.0   \n",
      "\n",
      "      Avg_PTS_Season  PTS_Per36_Season  Opponent_DEF_RATING  TS%_Roll_3  ...  \\\n",
      "3518       20.593750         25.871320                115.0    0.613634  ...   \n",
      "494         7.380952         10.127042                115.0    0.679239  ...   \n",
      "2834        2.142857         19.285714                115.0    0.339793  ...   \n",
      "2708       16.750000         18.843750                115.0    0.521528  ...   \n",
      "4861        2.153846         10.500000                115.0    0.280112  ...   \n",
      "\n",
      "      Avg_TS%_Season  Player_USG_Proxy_Roll_3  Player_USG_Proxy_Roll_5  \\\n",
      "3518        0.535738                 0.612045                 0.625996   \n",
      "494         0.622390                 0.229899                 0.241470   \n",
      "2834        0.456204                 1.086154                 0.851692   \n",
      "2708        0.540366                 0.596901                 0.610095   \n",
      "4861        0.555556                 0.497778                 0.432000   \n",
      "\n",
      "      Avg_USG%_Proxy_Season  Rest_Days  Is_Home  Actual_PTS  Predicted_PTS  \\\n",
      "3518               0.724144        2.0        0          20      21.555418   \n",
      "494                0.262287        3.0        1           8       6.974077   \n",
      "2834               0.658571        3.0        1           5       5.233140   \n",
      "2708               0.539512        2.0        1          31      17.564922   \n",
      "4861               0.314583        2.0        1           2       3.275616   \n",
      "\n",
      "          Error  Abs_Error  \n",
      "3518  -1.555418   1.555418  \n",
      "494    1.025923   1.025923  \n",
      "2834  -0.233140   0.233140  \n",
      "2708  13.435078  13.435078  \n",
      "4861  -1.275616   1.275616  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Largest Errors (Top 5):\n",
      "      PTS_Roll_3  PTS_Roll_5  MIN_Roll_3  MIN_Roll_5  FGA_Roll_3  FGA_Roll_5  \\\n",
      "2434    4.666667         2.8   10.666667         7.6    5.333333         3.2   \n",
      "3550    0.000000         0.0    0.000000         0.0    0.000000         0.0   \n",
      "1459   31.666667        32.2   35.666667        36.6   22.000000        22.0   \n",
      "2311   18.000000        17.2   30.333333        30.8   16.333333        15.0   \n",
      "1726   12.000000        12.6   35.666667        36.2   12.000000        11.8   \n",
      "\n",
      "      Avg_PTS_Season  PTS_Per36_Season  Opponent_DEF_RATING  TS%_Roll_3  ...  \\\n",
      "2434        5.685714         15.373391                115.0    0.462963  ...   \n",
      "3550        0.000000          0.000000                115.0    0.000000  ...   \n",
      "1459       26.843750         27.734529                115.0    0.624094  ...   \n",
      "2311       16.142857         21.373030                115.0    0.535594  ...   \n",
      "1726       10.876923         14.069652                115.0    0.441276  ...   \n",
      "\n",
      "      Avg_TS%_Season  Player_USG_Proxy_Roll_3  Player_USG_Proxy_Roll_5  \\\n",
      "2434        0.467312                 0.497899                 0.348739   \n",
      "3550        0.000000                 0.000000                 0.000000   \n",
      "1459        0.584863                 0.829790                 0.814927   \n",
      "2311        0.594320                 0.597379                 0.548032   \n",
      "1726        0.585614                 0.443120                 0.406223   \n",
      "\n",
      "      Avg_USG%_Proxy_Season  Rest_Days  Is_Home  Actual_PTS  Predicted_PTS  \\\n",
      "2434               0.540601        1.0        1          37       5.206250   \n",
      "3550               0.000000        2.0        1          36       7.089577   \n",
      "1459               0.756377        2.0        0           6      29.459410   \n",
      "2311               0.546760        3.0        1          40      17.889566   \n",
      "1726               0.379016        2.0        1          34      12.110507   \n",
      "\n",
      "          Error  Abs_Error  \n",
      "2434  31.793750  31.793750  \n",
      "3550  28.910423  28.910423  \n",
      "1459 -23.459410  23.459410  \n",
      "2311  22.110434  22.110434  \n",
      "1726  21.889493  21.889493  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Feature Importances (Tuned XGBoost):\n",
      "                    Feature  Importance\n",
      "6            Avg_PTS_Season    0.366826\n",
      "5                FGA_Roll_5    0.195897\n",
      "0                PTS_Roll_3    0.088226\n",
      "1                PTS_Roll_5    0.081995\n",
      "4                FGA_Roll_3    0.070996\n",
      "14    Avg_USG%_Proxy_Season    0.032643\n",
      "2                MIN_Roll_3    0.031589\n",
      "3                MIN_Roll_5    0.022774\n",
      "10               TS%_Roll_5    0.018879\n",
      "7          PTS_Per36_Season    0.015207\n",
      "9                TS%_Roll_3    0.014680\n",
      "11           Avg_TS%_Season    0.014384\n",
      "13  Player_USG_Proxy_Roll_5    0.014037\n",
      "12  Player_USG_Proxy_Roll_3    0.011847\n",
      "15                Rest_Days    0.010838\n",
      "16                  Is_Home    0.009183\n",
      "8       Opponent_DEF_RATING    0.000000\n"
     ]
    }
   ],
   "source": [
    "# --- Model Evaluation (Tuned XGBoost) ---\n",
    "# Pass X_train.columns for feature importance display\n",
    "train_cols = X_train.columns if X_train is not None else None\n",
    "evaluate_model(model_xgb_tuned, X_test, y_test, \"Tuned XGBoost\", X_train_cols=train_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba_prop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
