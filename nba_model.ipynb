{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from nba_api.stats.endpoints import playergamelog, commonplayerinfo, leaguegamefinder, leaguedashteamstats, leaguedashplayerstats\n",
    "from nba_api.stats.static import players, teams\n",
    "import time \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all players\n",
    "nba_players = players.get_players()\n",
    "players_df = pd.DataFrame(nba_players)\n",
    "print(f\"Total players found: {len(players_df)}\")\n",
    "players_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all teams\n",
    "nba_teams = teams.get_teams()\n",
    "teams_df = pd.DataFrame(nba_teams)\n",
    "print(f\"Total teams found: {len(teams_df)}\")\n",
    "teams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Team Defensive Stats for the season\n",
    "print(\"Fetching team defensive stats...\")\n",
    "team_def_stats_df = pd.DataFrame() \n",
    "try:\n",
    "    team_stats = leaguedashteamstats.LeagueDashTeamStats(\n",
    "        season='2023-24',\n",
    "        measure_type_detailed_defense='Defense' \n",
    "    )\n",
    "    temp_df = team_stats.get_data_frames()[0]\n",
    "    \n",
    "    print(\"Available columns in team stats:\")\n",
    "    print(temp_df.columns) \n",
    "\n",
    "    identifier_column = 'TEAM_NAME' \n",
    "    \n",
    "    if identifier_column not in temp_df.columns:\n",
    "         if 'TEAM_ABBREVIATION' in temp_df.columns:\n",
    "              identifier_column = 'TEAM_ABBREVIATION'\n",
    "              print(f\"Using '{identifier_column}' as identifier.\")\n",
    "         elif 'TEAM_ID' in temp_df.columns:\n",
    "              identifier_column = 'TEAM_ID'\n",
    "              print(f\"Using '{identifier_column}' as identifier.\")\n",
    "         else:\n",
    "              raise KeyError(f\"Could not find a suitable team identifier column. Available: {temp_df.columns}\")\n",
    "\n",
    "    team_def_stats_df = temp_df[[identifier_column, 'DEF_RATING']].copy()\n",
    "    \n",
    "    if identifier_column in ['TEAM_NAME', 'TEAM_ID'] and 'teams_df' in locals():\n",
    "         merge_left_col = 'TEAM_NAME' if identifier_column == 'TEAM_NAME' else 'TEAM_ID'\n",
    "         merge_right_col = 'full_name' if identifier_column == 'TEAM_NAME' else 'id'\n",
    "         \n",
    "         if identifier_column == 'TEAM_ID':\n",
    "             team_def_stats_df[identifier_column] = team_def_stats_df[identifier_column].astype(int)\n",
    "             teams_df['id'] = teams_df['id'].astype(int)\n",
    "\n",
    "         team_def_stats_df = pd.merge(team_def_stats_df, teams_df[['id', 'full_name', 'abbreviation']], left_on=merge_left_col, right_on=merge_right_col, how='left')\n",
    "         \n",
    "         if 'abbreviation' in team_def_stats_df.columns:\n",
    "             team_def_stats_df = team_def_stats_df[['abbreviation', 'DEF_RATING']].rename(columns={'abbreviation': 'TEAM_ABBREVIATION'})\n",
    "         else:\n",
    "              print(\"Warning: Could not find 'abbreviation' after merging with teams_df.\")\n",
    "              team_def_stats_df = pd.DataFrame() \n",
    "              \n",
    "    elif identifier_column == 'TEAM_ABBREVIATION':\n",
    "         pass \n",
    "    else:\n",
    "         print(f\"Warning: Identifier column '{identifier_column}' might require manual handling for merging later.\")\n",
    "\n",
    "\n",
    "    if not team_def_stats_df.empty:\n",
    "        print(\"\\nTeam defensive stats processed.\")\n",
    "        print(team_def_stats_df.head())\n",
    "    elif 'TEAM_ABBREVIATION' in temp_df.columns: \n",
    "         print(\"\\nProcessing stats using TEAM_ABBREVIATION directly.\")\n",
    "         team_def_stats_df = temp_df[['TEAM_ABBREVIATION', 'DEF_RATING']].copy()\n",
    "         print(team_def_stats_df.head())\n",
    "    else:\n",
    "         print(\"\\nCould not process team defensive stats correctly.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch specific SSL error if possible, otherwise general exception\n",
    "    if 'CERTIFICATE_VERIFY_FAILED' in str(e):\n",
    "         print(f\"\\nSSL Certificate Error fetching team stats: {e}\")\n",
    "         print(\"This might be due to a corporate network/proxy. Using fallback DEF_RATING.\")\n",
    "    else:\n",
    "         print(f\"\\nError fetching or processing team defensive stats: {e}\")\n",
    "    team_def_stats_df = pd.DataFrame() # Ensure it's empty on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get game logs for a player and season with delay\n",
    "def get_player_log(player_id, season='2023-24'):\n",
    "    print(f\"Fetching logs for player {player_id}...\")\n",
    "    try:\n",
    "        # Note: PlayerGameLog endpoint provides FGA, PTS, FTA, and TOV\n",
    "        log = playergamelog.PlayerGameLog(player_id=player_id, season=season)\n",
    "        df = log.get_data_frames()[0]\n",
    "        time.sleep(0.6) # NBA API rate limit\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching logs for player {player_id}: {e}\")\n",
    "        time.sleep(0.6)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --- Define Season and Output File ---\n",
    "SEASON = '2023-24'\n",
    "RAW_GAMELOG_FILE = f'nba_gamelogs_raw_{SEASON}.csv'\n",
    "MIN_MINUTES_THRESHOLD = 15 # Minimum average minutes per game to be included\n",
    "MAX_PLAYERS_TO_FETCH = 100 # Limit players for faster fetching if needed\n",
    "\n",
    "# --- Check if Processed Data Exists and Contains TOV ---\n",
    "FETCH_REQUIRED = False\n",
    "if os.path.exists(RAW_GAMELOG_FILE):\n",
    "    print(f\"Loading existing raw game logs from {RAW_GAMELOG_FILE}...\")\n",
    "    try:\n",
    "        all_gamelogs_df = pd.read_csv(RAW_GAMELOG_FILE)\n",
    "        # Ensure Player_ID is integer if loaded from CSV\n",
    "        if 'Player_ID' in all_gamelogs_df.columns:\n",
    "            all_gamelogs_df['Player_ID'] = all_gamelogs_df['Player_ID'].astype(int)\n",
    "        # Check if TOV column is present\n",
    "        if 'TOV' not in all_gamelogs_df.columns:\n",
    "            print(\"Warning: 'TOV' column missing from existing CSV. Re-fetching required.\")\n",
    "            FETCH_REQUIRED = True\n",
    "        else:\n",
    "            print(\"'TOV' column found in existing CSV.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or checking CSV file {RAW_GAMELOG_FILE}: {e}. Re-fetching required.\")\n",
    "        FETCH_REQUIRED = True\n",
    "        all_gamelogs_df = pd.DataFrame() # Ensure it's empty if loading failed\n",
    "else:\n",
    "    print(\"Raw game log file not found. Fetching data...\")\n",
    "    FETCH_REQUIRED = True\n",
    "    all_gamelogs_df = pd.DataFrame()\n",
    "\n",
    "# --- Fetch Data if Required ---\n",
    "if FETCH_REQUIRED:\n",
    "    # Remove old file if it exists but is incomplete\n",
    "    if os.path.exists(RAW_GAMELOG_FILE):\n",
    "        print(f\"Removing incomplete file: {RAW_GAMELOG_FILE}\")\n",
    "        try:\n",
    "            os.remove(RAW_GAMELOG_FILE)\n",
    "        except OSError as e:\n",
    "            print(f\"Error removing file: {e}\")\n",
    "            \n",
    "    # --- Filter Players Based on Season Stats (e.g., Minutes Played) ---\n",
    "    print(f\"Fetching player stats for {SEASON} to filter...\")\n",
    "    player_ids_to_fetch = []\n",
    "    try:\n",
    "        player_stats = leaguedashplayerstats.LeagueDashPlayerStats(season=SEASON)\n",
    "        player_stats_df = player_stats.get_data_frames()[0]\n",
    "        time.sleep(0.6)\n",
    "        \n",
    "        # Filter players playing significant minutes\n",
    "        relevant_players_df = player_stats_df[player_stats_df['MIN'] >= MIN_MINUTES_THRESHOLD]\n",
    "        player_ids_to_fetch = relevant_players_df['PLAYER_ID'].unique().tolist()\n",
    "        print(f\"Found {len(player_ids_to_fetch)} players averaging >= {MIN_MINUTES_THRESHOLD} MPG.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching player stats for filtering: {e}. Falling back to all active players.\")\n",
    "        # Fallback: Get all active players if stats fetch fails\n",
    "        active_players_df = players_df[players_df['is_active'] == True]\n",
    "        player_ids_to_fetch = active_players_df['id'].tolist()\n",
    "        print(f\"Fetching for all {len(player_ids_to_fetch)} active players (fallback).\")\n",
    "        \n",
    "    # --- Limit players if needed ---\n",
    "    if len(player_ids_to_fetch) > MAX_PLAYERS_TO_FETCH:\n",
    "        print(f\"Limiting fetch to {MAX_PLAYERS_TO_FETCH} players for speed.\")\n",
    "        player_ids_to_fetch = player_ids_to_fetch[:MAX_PLAYERS_TO_FETCH]\n",
    "        \n",
    "    # --- Fetching game logs for filtered players ---\n",
    "    if player_ids_to_fetch:\n",
    "        print(f\"Fetching game logs for {len(player_ids_to_fetch)} players...\")\n",
    "        fetched_logs = [] # Collect dataframes in a list first\n",
    "        for i, p_id in enumerate(player_ids_to_fetch):\n",
    "            print(f\"Progress: {i+1}/{len(player_ids_to_fetch)}\")\n",
    "            player_log_df = get_player_log(p_id, season=SEASON)\n",
    "            if not player_log_df.empty:\n",
    "                # Add Player_ID if it's missing (sometimes happens)\n",
    "                if 'Player_ID' not in player_log_df.columns:\n",
    "                     player_log_df['Player_ID'] = p_id\n",
    "                fetched_logs.append(player_log_df)\n",
    "                \n",
    "        # --- Concatenate and Save the fetched data ---\n",
    "        if fetched_logs:\n",
    "            all_gamelogs_df = pd.concat(fetched_logs, ignore_index=True)\n",
    "            print(f\"\\nSaving {len(all_gamelogs_df)} game logs to {RAW_GAMELOG_FILE}...\")\n",
    "            all_gamelogs_df.to_csv(RAW_GAMELOG_FILE, index=False)\n",
    "            print(\"Save complete.\")\n",
    "        else:\n",
    "            print(\"\\nNo game logs were fetched or concatenated.\")\n",
    "            all_gamelogs_df = pd.DataFrame() # Ensure it's an empty DF if nothing was fetched\n",
    "    else:\n",
    "        print(\"\\nNo player IDs identified for fetching.\")\n",
    "        all_gamelogs_df = pd.DataFrame()\n",
    "\n",
    "# --- Display results ---\n",
    "if not all_gamelogs_df.empty:\n",
    "    print(f\"\\nTotal game logs available: {len(all_gamelogs_df)}\")\n",
    "    print(f\"Unique players in logs: {all_gamelogs_df['Player_ID'].nunique()}\")\n",
    "    # Check essential columns after loading/fetching\n",
    "    for col in ['FTA', 'TOV']:\n",
    "        if col in all_gamelogs_df.columns:\n",
    "            print(f\"'{col}' column successfully included.\")\n",
    "        else:\n",
    "            print(f\"Warning: '{col}' column is missing from the loaded/fetched data!\")\n",
    "    print(all_gamelogs_df.head())\n",
    "else:\n",
    "    print(\"\\nall_gamelogs_df is empty. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preprocessing ---\n",
    "\n",
    "# Check if all_gamelogs_df exists and is not empty before proceeding\n",
    "if 'all_gamelogs_df' in locals() and not all_gamelogs_df.empty:\n",
    "    processed_df = all_gamelogs_df.copy()\n",
    "    processed_df['GAME_DATE'] = pd.to_datetime(processed_df['GAME_DATE'])\n",
    "\n",
    "    # Select relevant columns (including FGA, FTA, and TOV)\n",
    "    # Ensure all expected columns exist, handle missing ones if necessary\n",
    "    expected_cols = ['Player_ID', 'Game_ID', 'GAME_DATE', 'MATCHUP', 'WL', \n",
    "                     'MIN', 'PTS', 'REB', 'AST', 'FG3M', 'STL', 'BLK', 'TOV', 'FGA', 'FTA'] # Added TOV\n",
    "    available_cols = [col for col in expected_cols if col in processed_df.columns]\n",
    "    missing_cols = [col for col in expected_cols if col not in processed_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing expected columns: {missing_cols}. Proceeding with available columns.\")\n",
    "    processed_df = processed_df[available_cols]\n",
    "\n",
    "    # Ensure necessary columns for calculations are numeric, coercing errors\n",
    "    for col in ['PTS', 'FGA', 'FTA', 'MIN', 'TOV']: # Added TOV\n",
    "        if col in processed_df.columns:\n",
    "            processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')\n",
    "        else:\n",
    "             print(f\"Warning: Column {col} needed for processing is missing.\")\n",
    "             \n",
    "    # Drop rows where essential numeric columns became NaN after coercion\n",
    "    # Important: Check if TOV exists before adding to subset\n",
    "    dropna_subset = ['PTS', 'FGA', 'FTA', 'MIN']\n",
    "    if 'TOV' in processed_df.columns:\n",
    "        dropna_subset.append('TOV')\n",
    "    processed_df.dropna(subset=dropna_subset, inplace=True)\n",
    "\n",
    "    def parse_matchup(matchup_str):\n",
    "        if pd.isna(matchup_str):\n",
    "             return 'Unknown', 'Unknown'\n",
    "        if '@' in matchup_str:\n",
    "            parts = matchup_str.split(' @ ')\n",
    "            opponent = parts[1]\n",
    "            home_away = 'Away'\n",
    "        elif 'vs.' in matchup_str:\n",
    "            parts = matchup_str.split(' vs. ')\n",
    "            opponent = parts[1]\n",
    "            home_away = 'Home'\n",
    "        else: \n",
    "            opponent = 'Unknown'\n",
    "            home_away = 'Unknown'\n",
    "        return opponent, home_away\n",
    "\n",
    "    if 'MATCHUP' in processed_df.columns:\n",
    "        processed_df[['Opponent', 'Home_Away']] = processed_df['MATCHUP'].apply(\n",
    "            lambda x: pd.Series(parse_matchup(x))\n",
    "        )\n",
    "    else:\n",
    "        print(\"Warning: 'MATCHUP' column not found. Cannot determine Opponent or Home/Away.\")\n",
    "        processed_df['Opponent'] = 'Unknown'\n",
    "        processed_df['Home_Away'] = 'Unknown'\n",
    "\n",
    "    processed_df = processed_df.sort_values(by=['Player_ID', 'GAME_DATE'])\n",
    "\n",
    "    print(\"Data preprocessing complete.\")\n",
    "    print(processed_df.head())\n",
    "else:\n",
    "    print(\"Skipping Data Preprocessing because 'all_gamelogs_df' is not available or empty.\")\n",
    "    processed_df = pd.DataFrame() # Ensure processed_df exists even if empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Engineering ---\n",
    "\n",
    "# Check if processed_df exists and is not empty\n",
    "if 'processed_df' in locals() and not processed_df.empty:\n",
    "    print(\"Starting Feature Engineering...\")\n",
    "    \n",
    "    # Calculate Rest Days\n",
    "    processed_df['Rest_Days'] = processed_df.groupby('Player_ID')['GAME_DATE'].diff().dt.days\n",
    "    # Fill first game NaN with a reasonable value (e.g., average rest or a specific indicator)\n",
    "    processed_df['Rest_Days'].fillna(2, inplace=True) # Assuming 2 days rest for the first game as a default\n",
    "    print(\"Calculated Rest Days.\")\n",
    "\n",
    "    # Calculate Player Usage Rate Proxy (Requires TOV)\n",
    "    if 'FGA' in processed_df.columns and 'FTA' in processed_df.columns and 'TOV' in processed_df.columns and 'MIN' in processed_df.columns:\n",
    "        # Ensure TOV is numeric (should be from preprocessing, but double-check)\n",
    "        processed_df['TOV'] = pd.to_numeric(processed_df['TOV'], errors='coerce').fillna(0)\n",
    "        \n",
    "        usg_numerator = processed_df['FGA'] + 0.44 * processed_df['FTA'] + processed_df['TOV']\n",
    "        usg_denominator = processed_df['MIN']\n",
    "        processed_df['Player_USG_Proxy'] = np.where(usg_denominator == 0, 0, usg_numerator / usg_denominator)\n",
    "        processed_df['Player_USG_Proxy'].fillna(0, inplace=True)\n",
    "        processed_df['Player_USG_Proxy'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        print(\"Calculated Player Usage Rate Proxy.\")\n",
    "    else:\n",
    "        print(\"Warning: Could not calculate Player Usage Rate Proxy due to missing FGA, FTA, TOV, or MIN columns.\")\n",
    "        processed_df['Player_USG_Proxy'] = 0 # Assign default value\n",
    "        \n",
    "    # Calculate True Shooting Percentage (TS%)\n",
    "    if 'PTS' in processed_df.columns and 'FGA' in processed_df.columns and 'FTA' in processed_df.columns:\n",
    "        denominator = 2 * (processed_df['FGA'] + 0.44 * processed_df['FTA'])\n",
    "        processed_df['TS%'] = np.where(denominator == 0, 0, processed_df['PTS'] / denominator)\n",
    "        processed_df['TS%'].fillna(0, inplace=True)\n",
    "        processed_df['TS%'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        print(\"Calculated TS%.\")\n",
    "    else:\n",
    "        print(\"Warning: Could not calculate TS% due to missing PTS, FGA, or FTA columns.\")\n",
    "        processed_df['TS%'] = 0 # Assign default value\n",
    "\n",
    "    # Rolling Averages (Include new features)\n",
    "    cols_for_rolling = ['PTS', 'MIN', 'FGA', 'FTA', 'TS%', 'Player_USG_Proxy'] # Added Player_USG_Proxy\n",
    "    for col in cols_for_rolling:\n",
    "        if col in processed_df.columns:\n",
    "            # Ensure column is numeric before rolling calculation\n",
    "            processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')\n",
    "            processed_df[col].fillna(0, inplace=True) # Fill NaNs introduced by coercion\n",
    "            \n",
    "            processed_df[f'{col}_Roll_3'] = processed_df.groupby('Player_ID')[col].transform(\n",
    "                lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)\n",
    "            )\n",
    "            processed_df[f'{col}_Roll_5'] = processed_df.groupby('Player_ID')[col].transform(\n",
    "                lambda x: x.rolling(window=5, min_periods=1).mean().shift(1)\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found for rolling average calculation.\")\n",
    "\n",
    "    # Cumulative Season Averages (Shifted)\n",
    "    # Check required columns exist (including TOV for USG% calculation)\n",
    "    cum_avg_req_cols = ['PTS', 'MIN', 'TS%', 'FGA', 'FTA', 'TOV']\n",
    "    if all(col in processed_df.columns for col in cum_avg_req_cols):\n",
    "        processed_df['Cum_PTS'] = processed_df.groupby('Player_ID')['PTS'].transform(lambda x: x.expanding().sum().shift(1))\n",
    "        processed_df['Cum_MIN'] = processed_df.groupby('Player_ID')['MIN'].transform(lambda x: x.expanding().sum().shift(1))\n",
    "        processed_df['Cum_Games'] = processed_df.groupby('Player_ID').cumcount() \n",
    "        processed_df['Cum_FGA'] = processed_df.groupby('Player_ID')['FGA'].transform(lambda x: x.expanding().sum().shift(1))\n",
    "        processed_df['Cum_FTA'] = processed_df.groupby('Player_ID')['FTA'].transform(lambda x: x.expanding().sum().shift(1))\n",
    "        processed_df['Cum_TOV'] = processed_df.groupby('Player_ID')['TOV'].transform(lambda x: x.expanding().sum().shift(1))\n",
    "\n",
    "        processed_df['Avg_PTS_Season'] = (processed_df['Cum_PTS'] / processed_df['Cum_Games']).replace([np.inf, -np.inf, np.nan], 0)\n",
    "        processed_df['PTS_Per36_Season'] = (processed_df['Cum_PTS'] / processed_df['Cum_MIN'] * 36).replace([np.inf, -np.inf, np.nan], 0)\n",
    "        \n",
    "        # Cumulative TS% calculation\n",
    "        cum_ts_denominator = 2 * (processed_df['Cum_FGA'] + 0.44 * processed_df['Cum_FTA'])\n",
    "        processed_df['Avg_TS%_Season'] = np.where(cum_ts_denominator == 0, 0, processed_df['Cum_PTS'] / cum_ts_denominator)\n",
    "        processed_df['Avg_TS%_Season'].fillna(0, inplace=True)\n",
    "        processed_df['Avg_TS%_Season'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        \n",
    "        # Cumulative USG% Proxy calculation\n",
    "        cum_usg_numerator = processed_df['Cum_FGA'] + 0.44 * processed_df['Cum_FTA'] + processed_df['Cum_TOV']\n",
    "        cum_usg_denominator = processed_df['Cum_MIN']\n",
    "        processed_df['Avg_USG%_Proxy_Season'] = np.where(cum_usg_denominator == 0, 0, cum_usg_numerator / cum_usg_denominator)\n",
    "        processed_df['Avg_USG%_Proxy_Season'].fillna(0, inplace=True)\n",
    "        processed_df['Avg_USG%_Proxy_Season'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        \n",
    "        print(\"Calculated cumulative averages including TS% and USG% Proxy.\")\n",
    "    else:\n",
    "         missing_cum_cols = [col for col in cum_avg_req_cols if col not in processed_df.columns]\n",
    "         print(f\"Warning: One or more columns ({missing_cum_cols}) missing for cumulative average calculation.\")\n",
    "\n",
    "    # Other Features\n",
    "    if 'Home_Away' in processed_df.columns:\n",
    "        processed_df['Is_Home'] = processed_df['Home_Away'].apply(lambda x: 1 if x == 'Home' else 0)\n",
    "    else:\n",
    "        processed_df['Is_Home'] = 0 # Default if Home_Away is missing\n",
    "\n",
    "    # Merge Opponent Stats\n",
    "    # Use fallback DEF_RATING if team_def_stats_df is empty (due to fetch error)\n",
    "    DEFAULT_DEF_RATING = 115.0 \n",
    "    if 'team_def_stats_df' in locals() and not team_def_stats_df.empty and 'TEAM_ABBREVIATION' in team_def_stats_df.columns and 'Opponent' in processed_df.columns:\n",
    "        print(\"Merging fetched team defensive stats...\")\n",
    "        team_def_stats_to_merge = team_def_stats_df.rename(columns={\n",
    "            'TEAM_ABBREVIATION': 'Opponent',\n",
    "            'DEF_RATING': 'Opponent_DEF_RATING'\n",
    "        })\n",
    "        try:\n",
    "            processed_df['Opponent'] = processed_df['Opponent'].astype(team_def_stats_to_merge['Opponent'].dtype)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not align Opponent column types for merge: {e}\")\n",
    "            \n",
    "        processed_df = pd.merge(processed_df, team_def_stats_to_merge[['Opponent', 'Opponent_DEF_RATING']], on='Opponent', how='left')\n",
    "        \n",
    "        if 'Opponent_DEF_RATING' in processed_df.columns:\n",
    "             processed_df['Opponent_DEF_RATING'] = pd.to_numeric(processed_df['Opponent_DEF_RATING'], errors='coerce')\n",
    "             # Fill NaNs with the mean *of the successfully merged ratings* if possible, otherwise use default\n",
    "             avg_def_rating = processed_df['Opponent_DEF_RATING'].mean() \n",
    "             fill_value = avg_def_rating if not pd.isna(avg_def_rating) else DEFAULT_DEF_RATING\n",
    "             processed_df['Opponent_DEF_RATING'].fillna(fill_value, inplace=True)\n",
    "             print(f\"Opponent defensive stats merged. Filled NaNs with {fill_value:.1f}.\")\n",
    "        else:\n",
    "             print(\"Warning: 'Opponent_DEF_RATING' column not created after merge. Using default.\")\n",
    "             processed_df['Opponent_DEF_RATING'] = DEFAULT_DEF_RATING\n",
    "    else:\n",
    "        print(f\"Warning: Team defensive stats not available or Opponent column missing. Using default DEF_RATING: {DEFAULT_DEF_RATING}\")\n",
    "        processed_df['Opponent_DEF_RATING'] = DEFAULT_DEF_RATING\n",
    "\n",
    "    # Final Fill NA for engineered features\n",
    "    # Identify all potential feature columns created\n",
    "    feature_cols = [col for col in processed_df.columns if '_Roll_' in col or '_Season' in col or col == 'Is_Home' or col == 'Opponent_DEF_RATING' or col == 'Rest_Days']\n",
    "    processed_df[feature_cols] = processed_df[feature_cols].fillna(0)\n",
    "    print(\"Feature Engineering complete.\")\n",
    "    print(processed_df.head(10))\n",
    "else:\n",
    "    print(\"Skipping Feature Engineering because 'processed_df' is not available or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare Data for Modeling ---\n",
    "\n",
    "# Check if processed_df exists and is not empty\n",
    "if 'processed_df' in locals() and not processed_df.empty:\n",
    "    print(\"Preparing data for modeling...\")\n",
    "    # Define required columns based on features actually created\n",
    "    required_cols = []\n",
    "    # Add base features\n",
    "    base_features = ['PTS_Roll_3', 'PTS_Roll_5', 'MIN_Roll_3', 'MIN_Roll_5', \n",
    "                     'FGA_Roll_3', 'FGA_Roll_5', 'Avg_PTS_Season', 'PTS_Per36_Season',\n",
    "                     'Opponent_DEF_RATING']\n",
    "    for col in base_features:\n",
    "        if col in processed_df.columns:\n",
    "            required_cols.append(col)\n",
    "        else:\n",
    "            print(f\"Note: Feature '{col}' not found in processed_df, excluding from model requirements.\")\n",
    "            \n",
    "    # Add TS% features\n",
    "    ts_features = ['TS%_Roll_3', 'TS%_Roll_5', 'Avg_TS%_Season']\n",
    "    for col in ts_features:\n",
    "         if col in processed_df.columns:\n",
    "            required_cols.append(col)\n",
    "         else:\n",
    "            print(f\"Note: TS% Feature '{col}' not found in processed_df, excluding from model requirements.\")\n",
    "            \n",
    "    # Add new USG% and Rest features\n",
    "    new_features = ['Player_USG_Proxy_Roll_3', 'Player_USG_Proxy_Roll_5', 'Avg_USG%_Proxy_Season', 'Rest_Days']\n",
    "    for col in new_features:\n",
    "        if col in processed_df.columns:\n",
    "            required_cols.append(col)\n",
    "        else:\n",
    "            print(f\"Note: New Feature '{col}' not found in processed_df, excluding from model requirements.\")\n",
    "            \n",
    "    # Add target variable if it exists\n",
    "    target = 'PTS'\n",
    "    if target not in processed_df.columns:\n",
    "        print(f\"Error: Target variable '{target}' not found in processed_df. Cannot proceed with modeling.\")\n",
    "        model_df = pd.DataFrame()\n",
    "        X_train, X_test, y_train, y_test = [None]*4\n",
    "    else:\n",
    "        # Drop rows where target or any required feature is missing BEFORE creating model_df\n",
    "        # Also drop rows with insufficient history for rolling features (implicitly handled by fillna(0) then dropna)\n",
    "        model_df = processed_df.dropna(subset=[target] + required_cols).copy()\n",
    "\n",
    "        # Convert Opponent_DEF_RATING to numeric if it exists and wasn't already\n",
    "        if 'Opponent_DEF_RATING' in model_df.columns:\n",
    "            model_df['Opponent_DEF_RATING'] = pd.to_numeric(model_df['Opponent_DEF_RATING'], errors='coerce')\n",
    "            if model_df['Opponent_DEF_RATING'].isnull().any():\n",
    "                mean_def_rating = model_df['Opponent_DEF_RATING'].mean()\n",
    "                print(f\"Filling NaN Opponent_DEF_RATING with mean: {mean_def_rating}\")\n",
    "                model_df['Opponent_DEF_RATING'].fillna(mean_def_rating, inplace=True)\n",
    "        \n",
    "        # Check if enough data remains\n",
    "        if model_df.empty or len(model_df) < 10: # Arbitrary threshold for minimum data\n",
    "            print(\"Not enough data with required features and target to build a model.\")\n",
    "            X_train, X_test, y_train, y_test = [None]*4 \n",
    "        else:\n",
    "            # Define features based on columns actually present in model_df\n",
    "            features = required_cols + ['Is_Home'] # Add Is_Home if it exists\n",
    "            features = [f for f in features if f in model_df.columns] # Ensure all features exist\n",
    "            \n",
    "            print(f\"Using features: {features}\")\n",
    "            X = model_df[features]\n",
    "            y = model_df[target]\n",
    "            \n",
    "            # Check for NaN/inf in features or target before split\n",
    "            if X.isnull().values.any() or y.isnull().values.any() or np.isinf(X.values).any() or np.isinf(y.values).any():\n",
    "                 print(\"Warning: NaN or Inf values detected in features or target before train/test split. Attempting to fill with 0.\")\n",
    "                 X = X.fillna(0)\n",
    "                 y = y.fillna(0)\n",
    "                 X = X.replace([np.inf, -np.inf], 0)\n",
    "                 y = y.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "            \n",
    "            print(f\"Data prepared for modeling. Training set size: {X_train.shape[0]}, Testing set size: {X_test.shape[0]}\")\n",
    "else:\n",
    "    print(\"Skipping Data Preparation for Modeling because 'processed_df' is not available or empty.\")\n",
    "    X_train, X_test, y_train, y_test = [None]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Evaluation Function ---\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name, X_train_cols=None):\n",
    "    \"\"\"Calculates and prints evaluation metrics for a given model.\"\"\"\n",
    "    print(f\"\\n--- {model_name} Evaluation ---\")\n",
    "    \n",
    "    if model is None or X_test is None or y_test is None or X_test.empty:\n",
    "        print(f\"Skipping {model_name} evaluation as model was not trained or test data was missing.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred) \n",
    "        rmse = np.sqrt(mse) \n",
    "\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\") \n",
    "\n",
    "        # --- Basic Error Analysis ---\n",
    "        X_test_results = X_test.copy()\n",
    "        X_test_results['Actual_PTS'] = y_test\n",
    "        X_test_results['Predicted_PTS'] = y_pred\n",
    "        X_test_results['Error'] = X_test_results['Actual_PTS'] - X_test_results['Predicted_PTS']\n",
    "        X_test_results['Abs_Error'] = np.abs(X_test_results['Error'])\n",
    "        \n",
    "        print(f\"\\nSample Predictions vs Actual ({model_name}):\")\n",
    "        print(X_test_results.head())\n",
    "        \n",
    "        print(\"\\nLargest Errors (Top 5):\")\n",
    "        print(X_test_results.sort_values(by='Abs_Error', ascending=False).head())\n",
    "        \n",
    "        # Optional: Feature Importances (Specific to tree-based models like XGBoost)\n",
    "        if hasattr(model, 'feature_importances_') and X_train_cols is not None:\n",
    "            print(f\"\\nFeature Importances ({model_name}):\")\n",
    "            importances = pd.DataFrame({\n",
    "                'Feature': X_train_cols,\n",
    "                'Importance': model.feature_importances_\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            print(importances)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during {model_name} evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Building & Tuning (Ridge Regression) ---\n",
    "model_ridge = None\n",
    "if X_train is not None and y_train is not None and not X_train.empty:\n",
    "    print(\"Tuning Ridge Regression Model...\")\n",
    "    \n",
    "    param_grid_ridge = {'alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0]}\n",
    "    \n",
    "    ridge_estimator = Ridge()\n",
    "    grid_search_ridge = GridSearchCV(ridge_estimator, param_grid_ridge, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    \n",
    "    try:\n",
    "        grid_search_ridge.fit(X_train, y_train)\n",
    "        model_ridge = grid_search_ridge.best_estimator_\n",
    "        print(f\"Best alpha found for Ridge: {grid_search_ridge.best_params_['alpha']}\")\n",
    "        print(\"Tuned Ridge Regression Model training complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Ridge GridSearchCV: {e}\")\n",
    "        model_ridge = None # Ensure model is None if fitting fails\n",
    "else:\n",
    "    print(\"Skipping Ridge tuning due to lack of training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Evaluation (Tuned Ridge Regression) ---\n",
    "evaluate_model(model_ridge, X_test, y_test, \"Tuned Ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Building & Tuning (XGBoost) ---\n",
    "model_xgb_tuned = None\n",
    "if X_train is not None and y_train is not None and not X_train.empty:\n",
    "    print(\"\\n--- Tuning XGBoost Model ---\")\n",
    "    \n",
    "    # Define a smaller parameter grid for faster initial tuning\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [100, 200], # Number of boosting rounds\n",
    "        'max_depth': [3, 5], # Maximum depth of a tree\n",
    "        'learning_rate': [0.05, 0.1], # Step size shrinkage\n",
    "        'subsample': [0.7, 0.9], # Fraction of samples used per tree\n",
    "        'colsample_bytree': [0.7, 0.9] # Fraction of features used per tree\n",
    "        # Add other parameters like 'gamma', 'reg_alpha', 'reg_lambda' for more extensive tuning later\n",
    "    }\n",
    "    \n",
    "    xgb_estimator = XGBRegressor(random_state=42, objective='reg:squarederror') # Use squared error objective\n",
    "    \n",
    "    # Use GridSearchCV (can switch to RandomizedSearchCV for larger grids)\n",
    "    grid_search_xgb = GridSearchCV(\n",
    "        estimator=xgb_estimator,\n",
    "        param_grid=param_grid_xgb,\n",
    "        scoring='neg_mean_absolute_error', # Optimize for MAE\n",
    "        cv=3, # Use 3-fold CV for speed initially\n",
    "        n_jobs=-1, # Use all available CPU cores\n",
    "        verbose=1 # Print progress\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting XGBoost GridSearchCV...\")\n",
    "        grid_search_xgb.fit(X_train, y_train)\n",
    "        model_xgb_tuned = grid_search_xgb.best_estimator_\n",
    "        print(f\"\\nBest parameters found for XGBoost: {grid_search_xgb.best_params_}\")\n",
    "        print(f\"Best MAE score during CV: {-grid_search_xgb.best_score_:.2f}\")\n",
    "        print(\"Tuned XGBoost Model training complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during XGBoost GridSearchCV: {e}\")\n",
    "        model_xgb_tuned = None # Ensure model is None if fitting fails\n",
    "else:\n",
    "    print(\"Skipping XGBoost tuning due to lack of training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Evaluation (Tuned XGBoost) ---\n",
    "# Pass X_train.columns for feature importance display\n",
    "train_cols = X_train.columns if X_train is not None else None\n",
    "evaluate_model(model_xgb_tuned, X_test, y_test, \"Tuned XGBoost\", X_train_cols=train_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba_prop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
